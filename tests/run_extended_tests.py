"""
Test runner for extended test cases - tests NLP pipeline with 1000+ cases
Includes validation for edge cases, diacritics, case sensitivity
"""
import json
import sys
from pathlib import Path
from datetime import datetime
from collections import defaultdict

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core_nlp.pipeline import NLPPipeline


def normalize_text(text):
    """Normalize text for comparison"""
    if text is None:
        return None
    return text.lower().strip()


def compare_results(expected, actual, test_input):
    """Compare expected vs actual results - adjusted for pipeline output format"""
    errors = []
    
    # Check event (allow partial match for short events)
    if expected.get("event") not in [None, "event_extracted", "event_with_typo"]:
        exp_event = normalize_text(expected["event"])
        act_event = normalize_text(actual.get("event"))
        
        # More lenient event matching - check if actual is substring of expected or vice versa
        if exp_event and act_event:
            # If they're not equal, check partial match
            if exp_event != act_event:
                # Allow if actual event is at least 50% of expected or vice versa
                if not (exp_event in act_event or act_event in exp_event):
                    errors.append(f"Event mismatch: expected '{exp_event}', got '{act_event}'")
        elif exp_event != act_event:
            errors.append(f"Event mismatch: expected '{exp_event}', got '{act_event}'")
    
    # Check time - pipeline returns start_time (ISO), not time_str
    # Just verify that if time was expected, start_time exists
    exp_time = expected.get("time_str")
    act_start_time = actual.get("start_time")
    if exp_time not in [None, "time_extracted", "time_extracted_or_none", "time_as_words"]:
        if exp_time and not act_start_time:
            errors.append(f"Time missing: expected time but got None")
        elif not exp_time and act_start_time:
            errors.append(f"Unexpected time: expected None, got '{act_start_time}'")
    
    # Check location (more lenient - allow partial match)
    if expected.get("location") not in [None, "first_location_extracted"]:
        exp_loc = normalize_text(expected["location"])
        act_loc = normalize_text(actual.get("location"))
        
        if exp_loc and act_loc:
            # Allow partial match (location extraction may be shorter due to regex limits)
            if not (exp_loc.startswith(act_loc) or act_loc.startswith(exp_loc)):
                errors.append(f"Location mismatch: expected '{exp_loc}', got '{act_loc}'")
        elif exp_loc != act_loc:
            errors.append(f"Location mismatch: expected '{exp_loc}', got '{act_loc}'")
    
    # Check reminder (exact match required)
    exp_reminder = expected.get("reminder_minutes", 0)
    act_reminder = actual.get("reminder_minutes", 0)
    if exp_reminder != act_reminder:
        errors.append(f"Reminder mismatch: expected {exp_reminder}, got {act_reminder}")
    
    return errors


def run_tests(test_file, max_tests=None):
    """Run all tests from file"""
    print(f"Loading tests from: {test_file}")
    
    with open(test_file, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    if max_tests:
        test_cases = test_cases[:max_tests]
    
    print(f"Running {len(test_cases)} tests...")
    print("=" * 80)
    
    nlp = NLPPipeline()
    
    results = {
        "total": len(test_cases),
        "passed": 0,
        "failed": 0,
        "errors": [],
        "error_categories": defaultdict(int),
        "edge_case_results": defaultdict(lambda: {"passed": 0, "failed": 0})
    }
    
    for i, test_case in enumerate(test_cases, 1):
        test_input = test_case["input"]
        expected = test_case["expected"]
        
        # Detect edge case category
        category = "normal"
        if not test_input or test_input.isspace():
            category = "empty_input"
        elif len(test_input.split()) <= 2:
            category = "short_input"
        elif len(test_input) > 100:
            category = "long_input"
        elif any(char in test_input for char in "#@./"):
            category = "special_chars"
        elif test_input != test_input.lower() and test_input != test_input.upper():
            category = "mixed_case"
        
        try:
            # Extract using NLP pipeline
            result = nlp.process(test_input)
            
            # Compare results
            errors = compare_results(expected, result, test_input)
            
            if errors:
                results["failed"] += 1
                results["edge_case_results"][category]["failed"] += 1
                
                error_info = {
                    "test_number": i,
                    "input": test_input,
                    "expected": expected,
                    "actual": result,
                    "errors": errors,
                    "category": category
                }
                results["errors"].append(error_info)
                
                # Categorize error types
                for error in errors:
                    if "Event" in error:
                        results["error_categories"]["event_extraction"] += 1
                    elif "Time" in error:
                        results["error_categories"]["time_extraction"] += 1
                    elif "Location" in error:
                        results["error_categories"]["location_extraction"] += 1
                    elif "Reminder" in error:
                        results["error_categories"]["reminder_extraction"] += 1
                
                # Print first 10 failures in detail
                if results["failed"] <= 10:
                    print(f"\n‚ùå Test #{i} FAILED [{category}]")
                    print(f"   Input: {test_input}")
                    print(f"   Errors: {', '.join(errors)}")
                    print(f"   Expected: {expected}")
                    print(f"   Got: {result}")
            else:
                results["passed"] += 1
                results["edge_case_results"][category]["passed"] += 1
                
        except Exception as e:
            results["failed"] += 1
            results["edge_case_results"][category]["failed"] += 1
            results["error_categories"]["exception"] += 1
            
            error_info = {
                "test_number": i,
                "input": test_input,
                "expected": expected,
                "actual": None,
                "errors": [f"Exception: {str(e)}"],
                "category": category
            }
            results["errors"].append(error_info)
            
            if results["failed"] <= 10:
                print(f"\nüí• Test #{i} EXCEPTION [{category}]")
                print(f"   Input: {test_input}")
                print(f"   Error: {str(e)}")
        
        # Progress indicator
        if i % 100 == 0:
            print(f"Progress: {i}/{len(test_cases)} ({i*100//len(test_cases)}%)")
    
    return results


def print_summary(results):
    """Print test summary"""
    print("\n" + "=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)
    
    total = results["total"]
    passed = results["passed"]
    failed = results["failed"]
    pass_rate = (passed / total * 100) if total > 0 else 0
    
    print(f"\nTotal Tests: {total}")
    print(f"‚úÖ Passed: {passed} ({pass_rate:.2f}%)")
    print(f"‚ùå Failed: {failed} ({100-pass_rate:.2f}%)")
    
    print(f"\nError Categories:")
    for category, count in sorted(results["error_categories"].items(), key=lambda x: -x[1]):
        print(f"  - {category}: {count}")
    
    print(f"\nEdge Case Results:")
    for category, stats in sorted(results["edge_case_results"].items()):
        total_cat = stats["passed"] + stats["failed"]
        pass_rate_cat = (stats["passed"] / total_cat * 100) if total_cat > 0 else 0
        print(f"  - {category}: {stats['passed']}/{total_cat} passed ({pass_rate_cat:.1f}%)")
    
    if results["errors"]:
        print(f"\n‚ö†Ô∏è  Total Failed Cases: {len(results['errors'])}")
        print(f"Showing first 20 failures:\n")
        
        for error_info in results["errors"][:20]:
            print(f"Test #{error_info['test_number']} [{error_info['category']}]:")
            print(f"  Input: {error_info['input'][:80]}...")
            print(f"  Errors: {', '.join(error_info['errors'][:2])}")
            print()


def save_error_report(results, output_file):
    """Save detailed error report to file"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total": results["total"],
            "passed": results["passed"],
            "failed": results["failed"],
            "pass_rate": f"{results['passed']/results['total']*100:.2f}%"
        },
        "error_categories": dict(results["error_categories"]),
        "edge_case_results": {k: dict(v) for k, v in results["edge_case_results"].items()},
        "failed_tests": results["errors"]
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìÑ Detailed error report saved to: {output_file}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Run extended NLP test cases")
    parser.add_argument("--file", default="tests/extended_test_cases.json", help="Test file path")
    parser.add_argument("--max", type=int, help="Maximum number of tests to run")
    parser.add_argument("--report", default="tests/test_report.json", help="Error report output file")
    
    args = parser.parse_args()
    
    results = run_tests(args.file, args.max)
    print_summary(results)
    save_error_report(results, args.report)
    
    # Exit with error code if tests failed
    sys.exit(0 if results["failed"] == 0 else 1)
